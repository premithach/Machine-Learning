{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57bdd828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical solution:\n",
      "Slope: 1.1696969696969697\n",
      "Intercept: 1.2363636363636363\n",
      "Sum of Squared Errors (SSE): 5.624242424242423\n",
      "R-squared value: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "\n",
    "def linear_regression_analytical(x, y):\n",
    "    n = len(x)\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    slope = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean)**2)\n",
    "    intercept = y_mean - slope * x_mean\n",
    "    \n",
    "    y_pred = slope * x + intercept\n",
    "    \n",
    "    # Calculate sum of squared errors (SSE)\n",
    "    sse = np.sum((y - y_pred)**2)\n",
    "    \n",
    "    # Calculate R-squared value\n",
    "    ss_total = np.sum((y - y_mean)**2)\n",
    "    r_squared = 1 - (sse / ss_total)\n",
    "    \n",
    "    return slope, intercept, sse, r_squared\n",
    "\n",
    "slope_analytical, intercept_analytical, sse_analytical, r_squared_analytical = linear_regression_analytical(x, y)\n",
    "\n",
    "print(\"Analytical solution:\")\n",
    "print(\"Slope:\", slope_analytical)\n",
    "print(\"Intercept:\", intercept_analytical)\n",
    "print(\"Sum of Squared Errors (SSE):\", sse_analytical)\n",
    "print(\"R-squared value:\", r_squared_analytical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5053a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full-batch Gradient Descent:\n",
      "Slope: 1.170263693076768\n",
      "Intercept: 1.2328099487610318\n",
      "Sum of Squared Errors (SSE): 5.624278989977716\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "Slope: 1.302966596120438\n",
      "Intercept: 0.8560883898713284\n",
      "Sum of Squared Errors (SSE): 7.571038549968754\n"
     ]
    }
   ],
   "source": [
    "# Implementing Gradient Descent for Linear Regression\n",
    "def gradient_descent(x, y, learning_rate, iterations):\n",
    "    n = len(x)\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        y_pred = slope * x + intercept\n",
    "        error = y_pred - y\n",
    "        slope -= (2/n) * learning_rate * np.dot(error, x)\n",
    "        intercept -= (2/n) * learning_rate * np.sum(error)\n",
    "    return slope, intercept\n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "slope_full, intercept_full = gradient_descent(x, y, learning_rate, iterations)\n",
    "\n",
    "y_pred_full = slope_full * x + intercept_full\n",
    "sse_full = np.sum((y - y_pred_full)**2)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"Slope:\", slope_full)\n",
    "print(\"Intercept:\", intercept_full)\n",
    "print(\"Sum of Squared Errors (SSE):\", sse_full)\n",
    "\n",
    "def stochastic_gradient_descent(x, y, learning_rate, epsilon):\n",
    "    n = len(x)\n",
    "    slope = 0\n",
    "    intercept = 0\n",
    "    prev_sse = float('inf')\n",
    "    \n",
    "    while True:\n",
    "        for i in range(n):\n",
    "            y_pred = slope * x[i] + intercept\n",
    "            error = y_pred - y[i]\n",
    "            \n",
    "            slope -= learning_rate * 2 * error * x[i]\n",
    "            intercept -= learning_rate * 2 * error\n",
    "        \n",
    "        y_pred = slope * x + intercept\n",
    "        sse = np.sum((y - y_pred)**2)\n",
    "        \n",
    "        if abs(prev_sse - sse) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            prev_sse = sse\n",
    "            \n",
    "    return slope, intercept\n",
    "\n",
    "learning_rate = 0.01\n",
    "epsilon = 0.0001\n",
    "slope_stochastic, intercept_stochastic = stochastic_gradient_descent(x, y, learning_rate, epsilon)\n",
    "\n",
    "y_pred_stochastic = slope_stochastic * x + intercept_stochastic\n",
    "sse_stochastic = np.sum((y - y_pred_stochastic)**2)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"Slope:\", slope_stochastic)\n",
    "print(\"Intercept:\", intercept_stochastic)\n",
    "print(\"Sum of Squared Errors (SSE):\", sse_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c90d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical solution:\n",
      "Slope: -0.053044743626211754\n",
      "Intercept: 2.0685581690891466\n",
      "\n",
      "Full-batch Gradient Descent:\n",
      "Slope: -0.053042173539674956\n",
      "Intercept: 2.06855816560783\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "Slope: 0.9615068742330134\n",
      "Intercept: 1.8122495639827125\n"
     ]
    }
   ],
   "source": [
    "#Download Boston Housing Rate Dataset. Analyse the input attributes and find out the\n",
    "#attribute that best follow the linear relationship with the output price. Implement both the\n",
    "#analytic formulation and gradient descent (Full-batch, stochastic) on LMS loss\n",
    "#formulation to compute the coefficients of regression matrix and compare the results.\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "correlation_with_price = np.abs(np.corrcoef(X_scaled.T, y)[0, 1:])\n",
    "best_attribute_index = np.argmax(correlation_with_price)\n",
    "\n",
    "X_selected = X_scaled[:, best_attribute_index]\n",
    "slope_analytical = np.cov(X_selected, y)[0, 1] / np.var(X_selected)\n",
    "intercept_analytical = np.mean(y) - slope_analytical * np.mean(X_selected)\n",
    "\n",
    "def gradient_descent(X, y, learning_rate, iterations):\n",
    "    n = len(X)\n",
    "    slope = intercept = 0\n",
    "    for _ in range(iterations):\n",
    "        y_pred = slope * X + intercept\n",
    "        error = y_pred - y\n",
    "        slope -= (2/n) * learning_rate * np.dot(error, X).sum()\n",
    "        intercept -= (2/n) * learning_rate * error.sum()\n",
    "    return slope, intercept\n",
    "\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "slope_full, intercept_full = gradient_descent(X_selected, y, learning_rate, iterations)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, epsilon):\n",
    "    n = len(X)\n",
    "    slope = intercept = 0\n",
    "    prev_sse = float('inf')\n",
    "    while True:\n",
    "        for i in range(n):\n",
    "            y_pred = slope * X[i] + intercept\n",
    "            error = y_pred - y[i]\n",
    "            slope -= learning_rate * 2 * error * X[i]\n",
    "            intercept -= learning_rate * 2 * error\n",
    "        sse = np.sum((slope * X + intercept - y)**2)\n",
    "        if abs(prev_sse - sse) < epsilon:\n",
    "            break\n",
    "        prev_sse = sse\n",
    "    return slope, intercept\n",
    "\n",
    "epsilon = 0.0001\n",
    "slope_stochastic, intercept_stochastic = stochastic_gradient_descent(X_selected, y, learning_rate, epsilon)\n",
    "\n",
    "print(\"Analytical solution:\")\n",
    "print(\"Slope:\", slope_analytical)\n",
    "print(\"Intercept:\", intercept_analytical)\n",
    "\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"Slope:\", slope_full)\n",
    "print(\"Intercept:\", intercept_full)\n",
    "\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"Slope:\", slope_stochastic)\n",
    "print(\"Intercept:\", intercept_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f59b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
